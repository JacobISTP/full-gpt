{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "198609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_classic.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_classic.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.storage import LocalFileStore\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "map_doc_prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"\n",
    "     You will receive a chunk of text.\\n\n",
    "     If it contains information that helps answer the question, \n",
    "     extract the minimal relevant quote(s) verbatim.\\n\n",
    "     If not relevant, return an empty string.\\n\\n\n",
    "     CHUNK:        \n",
    "    ------\n",
    "    {context}\n",
    "    \"\"\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    results = []\n",
    "    for doc in documents:\n",
    "        result = map_doc_chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": question\n",
    "    }).content\n",
    "        results.append(result)\n",
    "\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "map_chain = {\"documents\":retriever, \"question\":RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\"\n",
    "    Given the following extracted parts of a long document and a question and chat history, create a final answer. If you don't know the answer, just sat that you don't know. Don't try to make up an answer.\n",
    "    \n",
    "    ------\n",
    "    {context}\n",
    "    \"\"\",),MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough(), \"history\":load_memory} | final_prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question)\n",
    "    memory.save_context({\"input\": question}, {\"ouput\":result.content})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb4772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The message he wrote on the table was: \"FREEDOM IS SLAVERY\" and \"TWO AND TWO MAKE FIVE\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 131, 'total_tokens': 160, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Crpf7LGY6H3dfbArGjts1CznFLaUH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b6630-7080-7280-a78a-9163b47ecf1f-0' usage_metadata={'input_tokens': 131, 'output_tokens': 29, 'total_tokens': 160, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "invoke_chain(\"What message did he write on the table? quote it exactly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21a32916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='What message did he write on the table? quote it exactly.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The message he wrote on the table was: \"FREEDOM IS SLAVERY\" and \"TWO AND TWO MAKE FIVE\"', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Is Aaronson guilty?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes, according to the extracted text, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Who is Julia?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Julia is a character in the text who is having a conversation with another character.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e310c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, according to the extracted text, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 235, 'total_tokens': 261, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Crpg0uHY2OATjN8AAbiTDhFJbeJNj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b6631-4582-7371-ba72-883951bc1eae-0' usage_metadata={'input_tokens': 235, 'output_tokens': 26, 'total_tokens': 261, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Is Aaronson guilty?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88b71729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Julia is a character in the text who is having a conversation with another character.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 266, 'total_tokens': 283, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CrpgQLhxRbeHDWHRHiCMyt1fivWDX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b6631-ab7a-73f0-8357-69d19bf7d9f5-0' usage_metadata={'input_tokens': 266, 'output_tokens': 17, 'total_tokens': 283, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Who is Julia?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
